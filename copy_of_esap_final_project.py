# -*- coding: utf-8 -*-
"""Copy of ESAP_final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-hTNVvkBPHqytSX47SRluGi6jupcTVQi

**This is what you'll need to submit on Gradescope:**

1. This notebook, completed.
2. Your data in csv form (as noted in the Data Cleaning section below).
3. Any other code you have written for your project.

# Data cleaning

The first step to data analysis is ensuring that you are focussing on the subset of the data that you have complete information about.

1. Having taken a look at your dataset, what columns and rows have you decided to drop from it? Why?

*A valid reason for dropping a column could be lack of information about the column, lack of a clear understanding of the units of measurement, a general feeling that it does not contain any useful information etc*

If you decided to focus on just a subset of your data please describe why you chose that subset and why you feel the other rows do not matter.

We have decided to drop the web description colums and the state column. we want to focus more on the other columns because we can extract more useful information from them to compare different aspects of each breach.

2. If you chose to merge on any additional datasets, include the code for that here.

*Answer in this markdown cell. Add more markdown cells if you want.*
"""

import numpy as np
import pandas as pd

breaches = pd.read_csv("https://raw.githubusercontent.com/goyal-pranav/ESAP-final-project/main/breach_report.csv")

breaches.head()

#breaches_clean = breaches.drop(["State"], axis = 1)

breaches_clean = breaches.drop(["Breach Submission Date"], axis = 1)

breaches_clean

breaches_clean = breaches_clean.drop(['Web Description'], axis = 1)

"""## Exploring the data

Using sorting, groupby etc find out some interesting aspects of the data. Even a short fact counts. For instance, if you were working with population data, you could say that 25% of the world lives in South Asia after you do some group by commands.

In this section of your project try to find as many interesting facts as possible.
"""

print(breaches_clean["State"].mode())

breaches_clean["Type of Breach"].mode()

grouped_by_type = breaches_clean.groupby(['Type of Breach'], as_index = False).agg(tot_affected = ("Individuals Affected", "sum"))
grouped_by_type

breaches_clean["Covered Entity Type"].mode()

breaches_clean["Business Associate Present"].mode()

num_No = breaches_clean['Business Associate Present'].value_counts()["No"]

num_No

"""# Visualizations """

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl

breaches_clean

"""Make at least 6 visualizations (if this number is unreasonable please talk to your mentoring TA before reducing it) that reveal something interesting about the data. Try to include at least one scatterplot and one histogram/bar graph (again, if the data does not lend itself to these plots do let us know

Make sure that your visualizations
1. Have things like the axes, titles, units etc
2. Are telling a slightly interesting story. Interesting = something a person who has not seen this data might not be able to just guess.

Write a few lines telling us what your visualization represents and what it reveals. Discuss any potential hypotheses that could result from these visualizations.

_Write your answer in this markdown cell._

A business associate, as defined by the HHS is:
a person or entity, other than a member of the workforce of a covered entity, who performs functions or activities on behalf of, or provides certain services to, a covered entity that involve access by the business associate to protected health information.  A “business associate” also is a subcontractor that creates, receives, maintains, or transmits protected health information on behalf of another business associate.

We will now analyse whether the presence of a business associate affects the number of data breaches
"""

breaches_clean_Aanika = breaches.drop(['Web Description'], axis = 1)

pie_chart = breaches_clean_Aanika.groupby("Business Associate Present", as_index = True).count()['Name of Covered Entity'].reset_index()

pie_chart.head()

plt.title("How often during a breach was a Business Associate Present")
plt.pie(pie_chart['Name of Covered Entity'], labels=pie_chart['Business Associate Present'])

def func(x):
    i = x.split('/')
    return '-'.join([i[2], i[0]])

breaches_clean_Aanika['Breach Submission Date'] = breaches_clean_Aanika['Breach Submission Date'].apply(lambda x : func(x))

new = breaches_clean_Aanika.sort_values("Breach Submission Date")

data = new.groupby('Breach Submission Date').sum()['Individuals Affected'][1:-1]

new

"""The scatterplot below shows the data breaches along with the month and year of the attack. We are able to vizualize this data and can attempt to notice trends. For example, the summer months (June, July, August) are the most common months in which data breaches occur"""

plt.scatter(x=data.index, y=data.values)
plt.rcParams["figure.figsize"] = [20,3]
plt.xlabel('Months', size = "large")
plt.ylabel('Number of Data Breaches', size = "large")

grouped_by_type.head()

"""We will now visualise the individuals affected by each type of data breach. We can clearly see that hacking and IT incidents are the most common type of data breach."""

plt.title("How many individuals were affected due to each type of data breach?")
plt.bar(grouped_by_type['Type of Breach'], height = grouped_by_type['tot_affected'])

"""We can visualise the above data with a pie chart and see the distribution of data breach methods

"""

pie = plt.pie(grouped_by_breach['tot_affected'], labels=None, autopct="%1.1f%%", pctdistance=1.5);
plt.legend(title = "Breaches")
labels= grouped_by_breach["Type of Breach"]
plt.title('Breaches', weight='bold', size=14)
plt.legend(pie[0],labels, bbox_to_anchor=(1.5,0), loc="right", fontsize=10, 
           bbox_transform=plt.gcf().transFigure)

grouped_by_state = breaches_clean.groupby(['State', 'Location of Breached Information'], as_index = False)

yeet = grouped_by_state.agg(num_incidents = ('Location of Breached Information', "count"))
max = {} #AK -> (name of breach, count)
for i, row in yeet.iterrows():
    state = row["State"]
    breach = row["Location of Breached Information"] 
    num_incidents = row["num_incidents"] 
    if state in max:
        count = (max[state])[1]
        if num_incidents > count:
            max[state] = (breach, num_incidents)
    else:
        max[state] = (breach, num_incidents) 
states = []
incidences = []
locations = []
for state, report in max.items():
    states.append(state)
    incidences.append(report[1]) 
    locations.append(report[0])

"""With the above data, we will visualise the number of unique breaches for the most common breaches in each state. For example, the most common breach in California is a Network Server breach, and there were 36 unique occurences of this breach in the last 24 months

The dataframe below shows the most commonly affected data location. The visualisation follows.
"""

data = {"State" : states, "Most Common Location" : locations}
df = pd.DataFrame(data)
df

plt.bar(states, incidences);
plt.xlabel("State");
plt.ylabel("Number of incidences of most common breach");

"""This bar graph shows the Entity Type compromised during the data breach along with the number of Individuals Affected from the data breach. This data is significant as it shows that entering as a heatlhcare provider would cause more individuals to be affected, therefore increasing security measures on such accounts would be a step in the right direction. """

grouped_by_type = breaches_clean.groupby(['Covered Entity Type'], as_index = False).agg(tot_affected = ("Individuals Affected", "sum"))
x = np.arange(len(grouped_by_type['tot_affected']))
y = grouped_by_type['tot_affected']
width = 0.35 
labels = list(grouped_by_type["Covered Entity Type"])
fig, ax = plt.subplots()
ax.bar(x - width/2, list(y), width)
ax.set_xticks(x)
ax.set_xticklabels(list(grouped_by_type["Covered Entity Type"]), rotation =45)
ax.set_ylabel('Individuals Affected')
ax.set_xlabel('Covered Entity Type')

